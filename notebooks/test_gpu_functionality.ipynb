{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "project_path = Path.cwd()\n",
    "while project_path.stem != 'gan-augmentation':\n",
    "    project_path = project_path.parent\n",
    "sys.path.append(str(project_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checks whether our PyTorch installation detects our CUDA installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get number of available GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get index of currently selected device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_index = torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get name of currently selected device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = torch.cuda.get_device_name(device_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get device by name / index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f'cuda:{device_index}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'> Using device: {device}\\n')\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('> Memory Usage:')\n",
    "    print(f'>>> Allocated:     {round(torch.cuda.memory_allocated(0) / 1024**3, 1)} GB')\n",
    "    print(f'>>> Max Allocated: {round(torch.cuda.max_memory_allocated(0) / 1024**3, 1)} GB')\n",
    "    print(f'>>> Cached:        {round(torch.cuda.memory_reserved(0) / 1024**3, 1)} GB')\n",
    "    print(f'>>> Max Cached:    {round(torch.cuda.max_memory_reserved(0) / 1024**3, 1)} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tensors and models with a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating / moving (copying) tensors on / to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(1, 2)                 # Default device is CPU\n",
    "t2 = torch.randn(1, 2).to(device)      # Copies a tensor to a GPU tensor\n",
    "t3 = torch.randn(1, 2).cuda()          # Copies a tensor to a GPU tensor\n",
    "t4 = torch.randn(1, 2, device=device)  # Creates a tensor on GPU directly\n",
    "print(t1)  # tensor([[..., ...]])\n",
    "print(t2)  # tensor([[..., ...]], device='cuda:0')\n",
    "print(t3)  # tensor([[..., ...]], device='cuda:0')\n",
    "print(t4)  # tensor([[..., ...]], device='cuda:0')\n",
    "\n",
    "t1.to(device)\n",
    "print(t1)  # tensor([[..., ...]]); .to() doesn't move the tensor in-place\n",
    "print(t1.is_cuda)  # False\n",
    "\n",
    "t1 = t1.to(device)\n",
    "print(t1)  # tensor([[..., ...]], device='cuda:0')\n",
    "print(t1.is_cuda) # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving models to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(1,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "\n",
    "model = M()      # default device is CPU\n",
    "model.to(device) # all model parameters have been moved on GPU; .to() moves the model in-place\n",
    "\n",
    "# Check whether all model parameters are on GPU\n",
    "model_is_cuda = all(param.is_cuda for param in model.parameters())\n",
    "if model_is_cuda:\n",
    "    print('Model is on GPU')\n",
    "else:\n",
    "    print('Model is on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving tensors from GPU to CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usually used to bring the model outputs to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "t1 = torch.randn(1, 2, device=device)\n",
    "t2 = t1.to('cpu')                      # Copies the tensor to a CPU tensor\n",
    "t3 = t1.cpu()                          # Copies the tensor to a CPU tensor\n",
    "print(t1)  # tensor([[..., ...]], device='cuda:0')\n",
    "print(t2)  # tensor([[..., ...]])\n",
    "print(t3)  # tensor([[..., ...]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
